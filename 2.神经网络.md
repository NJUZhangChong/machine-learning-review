## 多层感知机MLP

把感知机连起来就变成了感知机的网络
输入到隐藏是全连接，隐藏到输出是单项的

![image-20200417214707882](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200417214707882.png)

隐藏层的权值有好多种设置方法均可以解决异或问题

![image-20200417221327638](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200417221327638.png)

#### 为什么需要隐藏层神经元

![image-20200417221450830](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200417221450830.png)

隐藏层目的：隐层单元对输入数据做一次特征变化，变化后新的数据到输出层做线性分割，得到线性界面+后面的点

#### 如何计算隐藏层的权值？

利用反向传播算法

向前：分阶段，逐层计算输入和输出

向后：分阶段，逐层调整权值

# 反向传播

#### 误差反传要素:

误差定义；Delta规则；激活函数；反传学习的推导

#### 误差定义

经典感知机利用Hebb规则算的是误差和，多层感知机是误差平方和 1/2是为了计算方便

![image-20200417222134028](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200417222134028.png)

## Delta规则（重要）

多层感知机的误差跟权值和输入呈函数关系。

Delta规则希望找使误差最小的点对应得权值。在训练样本中，x是固定集合，在固定前提下，找w使得误差最小。所以让误差E针对x求导，使导数为0，等于0即是局部最小的点。

可行前提：激励函数是连续和可微分的

![image-20200417222731316](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200417222731316.png)



![image-20200417224341253](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200417224341253.png)

![image-20200417224402086](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200417224402086.png)

主要是多了对激活函数的求导

![image-20200417224942454](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200417224942454.png)

初始c大一点，学的快一点，后来小一点，不要在最优值附近震荡
delta不能克服单层网络不能用于非线性的局限，但它的可导性为后续提供支持

## 误差传播

![image-20200417225400524](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200417225400524.png)

单层网络HEbb,delta都可以（如果激活函数连续可微的话）
多层网络只可以用delta规则

## 反向传播算法

![image-20200417230345454](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200417230345454.png)

难点：输出层的误差和隐藏层有什么关系，即信用分配问题。误差是哪个组件为主产生的，或者大家都有贡献，哪个贡献大，哪个小

## BP神经网络

感知机模型变成多层就是多层感知机，在多层感知机用BP学习算法就变成了BP神经网络，BP神经网络本质是多层感知机。

![image-20200417230934907](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200417230934907.png)

反传学习不等于反馈

反传学习是误差；反馈是输入输出

多层感知机用的BP算法名气太大，大到用名字命名多层感知机

#### 感知机和多层感知机不同：

经典感知机利用Hebb规则算的是误差和，多层感知机是误差平方和

感知机的激活函数可以是阶跃函数不可微的，而多层感知机的必须激励函数是连续和可微分的

![image-20200418102614655](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418102614655.png)

## BP误差反传学习算法推导

![image-20200418111456312](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418111456312.png)

根据链式规则，对突触权值修正

![image-20200418111922165](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418111922165.png)

求得偏导数如下：

![image-20200418112008727](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418112008727.png)



另外一种表示

![image-20200418112049729](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418112049729.png)

另外2种情况：

![image-20200418112138256](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418112138256.png)

![image-20200418112301614](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418112301614.png)

中间不知道期望输出是什么，如何调节神经元j的权值

![image-20200418112342279](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418112342279.png)

yj对后面所有误差都有贡献，要算每一个ek对yj的偏导，而Wji只对第i个误差有贡献

![image-20200418114032240](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418114032240.png)

![image-20200418114238926](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418114238926.png)



![image-20200418112610288](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418112610288.png)

引入Sigmoid激活函数的好处

![image-20200418115201591](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418115201591.png)

![image-20200418115348821](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418115348821.png)

## 关于BP反向传播算法的其他议题

#### 初始权值

需要多次设定随机初始权值，设定方法有2种

1， 权值在w~（0,1）的高斯分布取

2，假设输入样本在样本空间均匀分布，和高斯分布的权值相乘得到一个随机量，n个权值，隐藏层高斯分布的方差变成0~n，导致输出层的输入接近于n,进而导致输出过大，激活函数饱和。

为不使隐藏层输出过大，必须对权值受限

![image-20200418122105538](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418122105538.png)

#### 激活函数

![image-20200418122321760](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418122321760.png)

#### 训练方法

![image-20200418122259121](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418122259121.png)

#### 停止机制

![image-20200418122357415](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200418122357415.png)

解决方案：先不设置停止机制，利用验证集观察误差变化，误差变化先小后大，先小是逐渐找到合适的机制，逐渐增大是因为在训练集上过拟合了，所以这个时候停止